{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your Deep Neural Network: Step by Step\n",
    "\n",
    "- In this notebook, we will implement all the functions required to build a deep neural network.\n",
    "- These functions will be imported into 'Deep Neural Network - Application.ipynb' through \n",
    "dnn_app_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from building_dnn_utils import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 - Outline\n",
    "\n",
    "- Initialize the parameters for a two-layer network and for an $L$-layer neural network.\n",
    "- Implement the forward propagation module (shown in purple in the figure below).\n",
    "     - Complete the LINEAR part of a layer's forward propagation step (resulting in $z^{[l]}$).\n",
    "     - ACTIVATION function (relu/sigmoid) is given in utils.\n",
    "     - Combine the previous two steps into a new [LINEAR->ACTIVATION] forward function.\n",
    "     - Stack the [LINEAR->RELU] forward function L-1 time (for layers 1 through L-1) and add a [LINEAR->SIGMOID] at the end (for the final layer $L$). This gives us a new L_model_forward function.\n",
    "- Compute the loss.\n",
    "- Implement the backward propagation module (denoted in red in the figure below).\n",
    "    - Complete the LINEAR part of a layer's backward propagation step.\n",
    "    - Gradient of the ACTIVATE function (relu_backward/sigmoid_backward) is given in utils.\n",
    "    - Combine the previous two steps into a new [LINEAR->ACTIVATION] backward function.\n",
    "    - Stack [LINEAR->RELU] backward L-1 times and add [LINEAR->SIGMOID] backward in a new L_model_backward function\n",
    "- Finally update the parameters.\n",
    "\n",
    "<img src=\"images/final outline.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center> **Figure 1**</center></caption><br>\n",
    "\n",
    "\n",
    "**Note** that for every forward function, there is a corresponding backward function. That is why at every step of your forward module we will be storing some values in a cache. The cached values are useful for computing gradients. In the backpropagation module we will then use the cache to calculate the gradients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3 - Initialization\n",
    "\n",
    "### 3.1 - 2-layer Neural Network\n",
    "\n",
    "**Outline**:\n",
    "- The model's structure is: *LINEAR -> RELU -> LINEAR -> SIGMOID*. \n",
    "- We use random initialization for the weight matrices `np.random.randn(shape).T * 0.01` with the correct shape.\n",
    "- We use zero initialization for the biases `np.zeros(shape)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_0, n_1, n_2):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_0 -- size of the input layer\n",
    "    n_1 -- size of the hidden layer\n",
    "    n_2 -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    w1 -- weight matrix of shape (n_0, n_1)\n",
    "                    b1 -- bias vector of shape (n_1, 1)\n",
    "                    w2 -- weight matrix of shape (n_1, n_2)\n",
    "                    b2 -- bias vector of shape (n_2, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1) # set up a seed so that output matches everytime\n",
    "    \n",
    "    w1 = np.random.randn(n_1, n_0).T * 0.01\n",
    "    b1 = np.zeros(shape=(n_1, 1))\n",
    "    w2 = np.random.randn(n_2, n_1).T * 0.01\n",
    "    b2 = np.zeros(shape=(n_2, 1))\n",
    "    \n",
    "    assert (w1.shape == (n_0, n_1))\n",
    "    assert (b1.shape == (n_1, 1))\n",
    "    assert (w2.shape == (n_1, n_2))\n",
    "    assert (b2.shape == (n_2, 1))\n",
    "    \n",
    "    parameters = {\"w1\": w1,\n",
    "                  \"b1\": b1,\n",
    "                  \"w2\": w2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 - L-layer Neural Network\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <td>  </td> \n",
    "        <td> **Shape of W** </td> \n",
    "        <td> **Shape of b**  </td> \n",
    "        <td> **Activation** </td>\n",
    "        <td> **Shape of Activation** </td> \n",
    "    <tr>\n",
    "       <tr>\n",
    "        <td> **Layer 1** </td> \n",
    "        <td> $(12288, n^{[1]})$ </td> \n",
    "        <td> $(n^{[1]},1)$ </td> \n",
    "        <td> $z^{[1]} = {w^{[1]}}^T  X + b^{[1]} $ </td> \n",
    "        <td> $(n^{[1]},209)$ </td> \n",
    "    <tr>\n",
    "       <tr>\n",
    "        <td> **Layer 2** </td> \n",
    "        <td> $(n^{[1]}, n^{[2]})$  </td> \n",
    "        <td> $(n^{[2]},1)$ </td> \n",
    "        <td>$z^{[2]} = {w^{[2]}}^T a^{[1]} + b^{[2]}$ </td> \n",
    "        <td> $(n^{[2]}, 209)$ </td> \n",
    "    <tr> \n",
    "       <tr>\n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$</td> \n",
    "        <td> $\\vdots$  </td> \n",
    "    <tr>    \n",
    "   <tr>\n",
    "        <td> **Layer L-1** </td> \n",
    "        <td> $(n^{[L-2]}, n^{[L-1]})$ </td> \n",
    "        <td> $(n^{[L-1]}, 1)$  </td> \n",
    "        <td>$z^{[L-1]} =  {w^{[L-1]}}^T a^{[L-2]} + b^{[L-1]}$ </td> \n",
    "        <td> $(n^{[L-1]}, 209)$ </td> \n",
    "    <tr>   \n",
    "   <tr>\n",
    "        <td> **Layer L** </td> \n",
    "        <td> $(n^{[L-1]}, n^{[L]})$ </td> \n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $z^{[L]} =  {w^{[L]}}^T a^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td> \n",
    "    <tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize_parameters_deep\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"w1\", \"b1\", ..., \"wL\", \"bL\":\n",
    "                    wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L): \n",
    "        parameters['w' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]).T * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros(shape=(layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['w' + str(l)].shape == (layer_dims[l-1], layer_dims[l]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Forward propagation module\n",
    "\n",
    "### 4.1 - Linear Forward \n",
    "\n",
    "The linear forward module (vectorized over all the examples) computes the following equations:\n",
    "\n",
    "$$z^{[l]} = {w^{[l]}}^T a^{[l-1]} +b^{[l]}\\tag{1}$$\n",
    "\n",
    "where $a^{[0]} = X$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_forward\n",
    "def linear_forward(a , w, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    a -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    w -- weights matrix: numpy array of shape (size of previous layer, size of current layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"a\", \"w\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    z = np.dot(w.T, a) + b\n",
    "    \n",
    "    cache = (a, w, b)\n",
    "    \n",
    "    return z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Linear-Activation Forward\n",
    "\n",
    "We use two activation functions from utils:\n",
    "\n",
    "- **Sigmoid**: $\\sigma(z) = \\sigma(w^T a + b) = \\frac{1}{ 1 + e^{-(w^T a + b)}}$. This function returns **two** items: the activation value \"`a`\" and a \"`cache`\" that contains \"`z`\" (it's what we will feed in to the corresponding backward function). To use it we can call: \n",
    "``` python\n",
    "a, activation_cache = sigmoid(z)\n",
    "```\n",
    "\n",
    "- **ReLU**: The mathematical formula for ReLu is $a = RELU(z) = max(0, z)$. We use the `relu` function from utils. This function returns **two** items: the activation value \"`a`\" and a \"`cache`\" that contains \"`z`\" (it's what we will feed in to the corresponding backward function). To use it we can call:\n",
    "``` python\n",
    "a, activation_cache = relu(z)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_activation_forward\n",
    "def linear_activation_forward(a_prev, w, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    a_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    w -- weights matrix: numpy array of shape ( size of previous layer, size of current layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    a -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        z, linear_cache = linear_forward(a_prev, w, b)\n",
    "        a, activation_cache = sigmoid(z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        z, linear_cache = linear_forward(a_prev, w, b)\n",
    "        a, activation_cache = relu(z)\n",
    "\n",
    "    \n",
    "    assert (a.shape == (w.shape[1], a_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return a, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) L-Layer Model \n",
    "\n",
    "For even more convenience when implementing the $L$-layer Neural Net, we will need a function that replicates the previous one (`linear_activation_forward` with RELU) $L-1$ times, then follows that with one `linear_activation_forward` with SIGMOID.\n",
    "\n",
    "<img src=\"images/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "<caption><center> **Figure 2** : *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model</center></caption><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L_model_forward\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    aL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    a = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        a_prev = a \n",
    "        a, cache = linear_activation_forward(a_prev, parameters[\"w\" + str(l)], parameters[\"b\" + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    aL, cache = linear_activation_forward(a, parameters[\"w\" + str(L)], parameters[\"b\" + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(aL.shape == (1, X.shape[1]))\n",
    "            \n",
    "    return aL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Cost function\n",
    "\n",
    "The cross-entropy cost $J$, is given by: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_cost\n",
    "def compute_cost(aL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    aL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    if Y.shape[1] > 1:\n",
    "        Y = Y.T # keeping vectors as column vectors\n",
    "\n",
    "    # Compute loss from aL and y\n",
    "    cost = -(1/m) * ( np.dot(Y.T, np.log(aL.T)) + np.dot((1-Y).T, np.log(1-aL.T)) )[0, 0]\n",
    "    \n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Backward propagation module\n",
    "\n",
    "<img src=\"images/backprop_kiank.png\" style=\"width:650px;height:250px;\">\n",
    "<caption><center> **Figure 3** : Forward and Backward propagation for *LINEAR->RELU->LINEAR->SIGMOID* <br> *The purple blocks represent the forward propagation, and the red blocks represent the backward propagation.*  </center></caption>\n",
    "\n",
    "<!-- \n",
    "For those of you who are expert in calculus (you don't need to be to do this assignment), the chain rule of calculus can be used to derive the derivative of the loss $\\mathcal{L}$ with respect to $z^{[1]}$ in a 2-layer network as follows:\n",
    "\n",
    "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n",
    "\n",
    "In order to calculate the gradient $dW^{[1]} = \\frac{\\partial L}{\\partial W^{[1]}}$, you use the previous chain rule and you do $dW^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$. During the backpropagation, at each step you multiply your current gradient by the gradient corresponding to the specific layer to get the gradient you wanted.\n",
    "\n",
    "Equivalently, in order to calculate the gradient $db^{[1]} = \\frac{\\partial L}{\\partial b^{[1]}}$, you use the previous chain rule and you do $db^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$.\n",
    "\n",
    "This is why we talk about **backpropagation**.\n",
    "!-->\n",
    "\n",
    "Outline:\n",
    "- LINEAR backward\n",
    "- LINEAR -> ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation\n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward (whole model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 - Linear backward\n",
    "\n",
    "For layer $l$, the linear part is: $z^{[l]} = {w^{[l]}}^T a^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
    "\n",
    "From chain rule, if we have $dz^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial z^{[l]}}$, then, to get $(dw^{[l]}, db^{[l]} da^{[l-1]})$ we have:\n",
    "\n",
    "<img src=\"images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
    "<caption><center> **Figure 4** </center></caption>\n",
    "\n",
    "$$ dz^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial w^{[l]}} = \\frac{1}{m} a^{[l-1]} {dz^{[l]}}^T \\tag{3}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dz^{[l](i)}\\tag{4}$$\n",
    "$$ da^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial a^{[l-1]}} = w^{[l]} dz^{[l]} \\tag{5}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_backward\n",
    "def linear_backward(dz, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dz -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (a_prev, w, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    da_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dw -- Gradient of the cost with respect to W (current layer l), same shape as w\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    a_prev, w, b = cache\n",
    "    m = a_prev.shape[1]\n",
    "\n",
    "    dw = (1/m) * np.dot(a_prev, dz.T)\n",
    "    db = (1/m) * np.sum(dz, axis=1, keepdims=True)\n",
    "    da_prev = np.dot(w, dz)\n",
    "    \n",
    "    assert (da_prev.shape == a_prev.shape)\n",
    "    assert (dw.shape == w.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return da_prev, dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Linear-Activation backward\n",
    "\n",
    "Next, we will create a function that merges the two helper functions: **`linear_backward`** and the backward step for the activation **`linear_activation_backward`**. \n",
    "\n",
    "We use following two backward functions from utils:\n",
    "- **`sigmoid_backward`**: Implements the backward propagation for SIGMOID unit. We call it as follows:\n",
    "\n",
    "```python\n",
    "dz = sigmoid_backward(da, activation_cache)\n",
    "```\n",
    "\n",
    "- **`relu_backward`**: Implements the backward propagation for RELU unit. We call it as follows:\n",
    "\n",
    "```python\n",
    "dz = relu_backward(da, activation_cache)\n",
    "```\n",
    "\n",
    "If $g(.)$ is the activation function, \n",
    "`sigmoid_backward` and `relu_backward` compute $$dz^{[l]} = da^{[l]} * g'(z^{[l]}) \\tag{6}$$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_activation_backward\n",
    "def linear_activation_backward(da, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    da -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    da_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as a_prev\n",
    "    dw -- Gradient of the cost with respect to w (current layer l), same shape as w\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dz = relu_backward(da, activation_cache)\n",
    "        da_prev, dw, db = linear_backward(dz, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dz = sigmoid_backward(da, activation_cache)\n",
    "        da_prev, dw, db = linear_backward(dz, linear_cache)\n",
    "    \n",
    "    return da_prev, dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 - L-Model Backward \n",
    "\n",
    "Now we will implement the backward function for the whole network. Recall that when we implemented the `L_model_forward` function, at each iteration, you stored a cache which contains (a_prev,w,b, and z). In the back propagation module, we will use those variables to compute the gradients. Therefore, in the `L_model_backward` function, we will iterate through all the hidden layers backward, starting from layer $L$. On each step, we will use the cached values for layer $l+1$ to backpropagate through layer $l$.\n",
    "\n",
    "\n",
    "<img src=\"images/mn_backward.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center>  **Figure 5** : Backward pass  </center></caption>\n",
    "\n",
    "** Initializing backpropagation**:\n",
    "To backpropagate through this network, we know that the output is, \n",
    "$a^{[L]} = \\sigma(z^{[L]})$. Our code thus needs to compute `daL` $= \\frac{\\partial \\mathcal{L}}{\\partial a^{[L]}}$ from this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L_model_backward\n",
    "def L_model_backward(aL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    aL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"da\" + str(l)] = ... \n",
    "             grads[\"dw\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = aL.shape[1]\n",
    "    if Y.shape[1] > 1:\n",
    "        Y = Y.T\n",
    "   \n",
    "    # Initializing the backpropagation\n",
    "    daL = - ( Y/aL.T - (1-Y)/(1-aL.T) ).T\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"da\" + str(L)], grads[\"dw\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(daL, current_cache, \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        da_prev_temp, dw_temp, db_temp = linear_activation_backward(grads[\"da\" + str(l + 2)], current_cache, \"relu\")\n",
    "        grads[\"da\" + str(l + 1)] = da_prev_temp\n",
    "        grads[\"dw\" + str(l + 1)] = dw_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 - Update Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_parameters\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"w\" + str(l+1)] = parameters[\"w\" + str(l+1)] - learning_rate * grads[\"dw\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7 - What we'll do next in 'Deep Neural Network - Application.ipynb'\n",
    "\n",
    "We will:\n",
    "- export all the above helper functions to 'dnn_app_utils.py' \n",
    "- import 'dnn_app_utils.py' into 'Deep Neural Network - Application.ipynb'\n",
    "- implement a two-layer neural network on 'catvnoncat' data\n",
    "- implement an L-layer neural network on 'catvnoncat' data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
